import os, json, torch
from transformers import MT5Tokenizer, MT5ForConditionalGeneration
from tqdm import tqdm
import sacrebleu
import argparse

# å°è¯•å¯¼å…¥ PEFT
try:
    from peft import PeftModel, PeftConfig
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False

os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_HUB_OFFLINE"] = "1"

def main():
    parser = argparse.ArgumentParser()
    # --- ä¿®æ”¹ï¼šå‚æ•°åæ›´é€šç”¨ ---
    parser.add_argument('--model_path', default='./runs/20251218_121749_mt5-finetune-raw_google-mt5-small_lora_raw/best_model', help="æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„ æˆ– .pt æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„")
    # --- æ–°å¢ï¼šå½“è¯„ä¼° .pt æ–‡ä»¶æ—¶ï¼Œéœ€è¦æŒ‡å®šåŸºç¡€æ¨¡å‹ ---
    parser.add_argument('--base_model_name', default='google/mt5-small', help="åŸºç¡€æ¨¡å‹åç§°")
    parser.add_argument('--data_dir', default='./data')
    parser.add_argument('--test_file', default='trian_test.jsonl', help="æµ‹è¯•æ–‡ä»¶å")
    parser.add_argument('--max_len', type=int, default=80, help="ç”Ÿæˆæœ€å¤§é•¿åº¦")
    parser.add_argument('--num_beams', type=int, default=4, help="Beam search çš„ beam æ•°é‡")
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu')
    parser.add_argument('--cache_dir', default='./hf_cache', help="HuggingFace ç¼“å­˜ç›®å½•")
    args = parser.parse_args()

    # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ™ºèƒ½åŠ è½½æ¨¡å‹ ---
    if os.path.isdir(args.model_path):
        print(f"åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶å¤¹: {args.model_path}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ LoRA æ¨¡å‹ (å­˜åœ¨ adapter_config.json)
        is_lora = os.path.exists(os.path.join(args.model_path, "adapter_config.json"))
        
        if is_lora:
            if not PEFT_AVAILABLE:
                raise ImportError("æ£€æµ‹åˆ° LoRA æ¨¡å‹ï¼Œä½†æœªå®‰è£… peft åº“ã€‚")
            
            print("ğŸš€ æ£€æµ‹åˆ° LoRA é€‚é…å™¨ï¼Œæ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹å’Œé€‚é…å™¨...")
            
            # 1. åŠ è½½åŸºç¡€æ¨¡å‹
            # å°è¯•ä» adapter_config.json è·å–åŸºç¡€æ¨¡å‹åç§°ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ args.base_model_name
            try:
                config = PeftConfig.from_pretrained(args.model_path)
                base_model_path = config.base_model_name_or_path
            except Exception:
                base_model_path = args.base_model_name
            
            print(f"åŸºç¡€æ¨¡å‹: {base_model_path} (Cache: {args.cache_dir})")
            
            # --- æ ¸å¿ƒä¿®å¤ï¼šæ‰‹åŠ¨å¯»æ‰¾ç¼“å­˜çš„å¿«ç…§è·¯å¾„ ---
            # å¦‚æœ base_model_path æ˜¯æ¨¡å‹åï¼ˆå¦‚ google/mt5-smallï¼‰ï¼Œå°è¯•æ‰¾åˆ°å…¶åœ¨ cache ä¸­çš„çœŸå®è·¯å¾„
            if not os.path.exists(base_model_path):
                model_cache_name = "models--" + base_model_path.replace("/", "--")
                snapshot_dir = os.path.join(args.cache_dir, model_cache_name, "snapshots")
                
                if os.path.exists(snapshot_dir):
                    snapshots = os.listdir(snapshot_dir)
                    
                    # å¯»æ‰¾åŒ…å«é…ç½®æ–‡ä»¶çš„å¿«ç…§
                    config_snap = None
                    for snap in snapshots:
                        if os.path.exists(os.path.join(snapshot_dir, snap, "config.json")):
                            config_snap = os.path.join(snapshot_dir, snap)
                            break
                    
                    # å¯»æ‰¾åŒ…å«æƒé‡çš„å¿«ç…§
                    weight_snap = None
                    weight_file = None
                    for snap in snapshots:
                        s_path = os.path.join(snapshot_dir, snap)
                        if os.path.exists(os.path.join(s_path, "model.safetensors")):
                            weight_snap = s_path
                            weight_file = "model.safetensors"
                            break
                        elif os.path.exists(os.path.join(s_path, "pytorch_model.bin")):
                            weight_snap = s_path
                            weight_file = "pytorch_model.bin"
                            break
                    
                    print(f"ğŸ” ç¼“å­˜åˆ†æ: Configåœ¨ {os.path.basename(config_snap) if config_snap else 'None'}, Weightsåœ¨ {os.path.basename(weight_snap) if weight_snap else 'None'}")

                    if config_snap and weight_snap:
                        if config_snap == weight_snap:
                            # å®Œç¾æƒ…å†µï¼šéƒ½åœ¨åŒä¸€ä¸ªç›®å½•
                            base_model_path = config_snap
                            tokenizer = MT5Tokenizer.from_pretrained(base_model_path, local_files_only=True)
                            model = MT5ForConditionalGeneration.from_pretrained(base_model_path, local_files_only=True)
                        else:
                            # åˆ†è£‚æƒ…å†µï¼šæ‰‹åŠ¨æ‹¼æ¥
                            print("âš ï¸ æ£€æµ‹åˆ°ç¼“å­˜åˆ†è£‚ï¼ˆConfigå’ŒWeightsåœ¨ä¸åŒç›®å½•ï¼‰ï¼Œæ­£åœ¨æ‰‹åŠ¨ç»„è£…...")
                            from transformers import AutoConfig
                            from safetensors.torch import load_file
                            
                            tokenizer = MT5Tokenizer.from_pretrained(config_snap, local_files_only=True)
                            config = AutoConfig.from_pretrained(config_snap, local_files_only=True)
                            model = MT5ForConditionalGeneration(config)
                            
                            weight_path = os.path.join(weight_snap, weight_file)
                            if weight_file.endswith(".safetensors"):
                                state_dict = load_file(weight_path)
                            else:
                                state_dict = torch.load(weight_path, map_location="cpu")
                            
                            model.load_state_dict(state_dict, strict=False)
                    else:
                        # æ— æ³•ä¿®å¤ï¼Œå›é€€åˆ°é»˜è®¤é€»è¾‘ï¼ˆå¯èƒ½ä¼šæŠ¥é”™ï¼‰
                        print("âŒ æ— æ³•åœ¨ç¼“å­˜ä¸­æ‰¾åˆ°å®Œæ•´çš„æ¨¡å‹æ–‡ä»¶")
                        if config_snap: base_model_path = config_snap # è‡³å°‘å°è¯•åŠ è½½config
            
            if 'model' not in locals(): # å¦‚æœä¸Šé¢æ²¡æœ‰æˆåŠŸåŠ è½½ model
                tokenizer = MT5Tokenizer.from_pretrained(base_model_path, local_files_only=True)
                model = MT5ForConditionalGeneration.from_pretrained(base_model_path, local_files_only=True)
            
            # 2. åŠ è½½ LoRA é€‚é…å™¨
            model = PeftModel.from_pretrained(model, args.model_path, local_files_only=True)
            
        else:
            # å…¨é‡å¾®è°ƒæ¨¡å‹
            print("ğŸš€ åŠ è½½å…¨é‡å¾®è°ƒæ¨¡å‹...")
            tokenizer = MT5Tokenizer.from_pretrained(args.model_path, local_files_only=True)
            model = MT5ForConditionalGeneration.from_pretrained(args.model_path, use_safetensors=True, local_files_only=True)
            
        model.to(args.device)

    elif args.model_path.endswith('.pt'):
        # æƒ…å†µ 2: è¾“å…¥æ˜¯ .pt æ£€æŸ¥ç‚¹æ–‡ä»¶
        print(f"åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶: {args.model_path}")
        print(f"ä½¿ç”¨åŸºç¡€æ¨¡å‹ '{args.base_model_name}' æ„å»ºç»“æ„ (Cache: {args.cache_dir})")
        
        # å…ˆåŠ è½½åŸºç¡€æ¨¡å‹ç»“æ„å’Œåˆ†è¯å™¨
        tokenizer = MT5Tokenizer.from_pretrained(args.base_model_name, cache_dir=args.cache_dir, local_files_only=True)
        model = MT5ForConditionalGeneration.from_pretrained(args.base_model_name, cache_dir=args.cache_dir, local_files_only=True)
        
        # åŠ è½½æ£€æŸ¥ç‚¹ä¸­çš„æƒé‡
        checkpoint = torch.load(args.model_path, map_location=args.device)
        
        # å¤„ç†å¯èƒ½çš„ key ä¸åŒ¹é… (ä¾‹å¦‚å¸¦æœ‰ 'module.' å‰ç¼€)
        state_dict = checkpoint['model_state_dict']
        # å¦‚æœæ˜¯ LoRA çš„ checkpointï¼Œè¿™é‡Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä½†é€šå¸¸ .pt æ˜¯å…¨é‡ä¿å­˜æˆ–è€…åªä¿å­˜äº† adapter
        # å‡è®¾è¿™é‡Œæ˜¯å…¨é‡æˆ–è€…ç”¨æˆ·çŸ¥é“è‡ªå·±åœ¨åšä»€ä¹ˆ
        
        model.load_state_dict(state_dict, strict=False)
        model.to(args.device)
    else:
        raise ValueError("æ— æ•ˆçš„ --model_pathï¼Œå¿…é¡»æ˜¯æ–‡ä»¶å¤¹æˆ– .pt æ–‡ä»¶")

    model.eval()

    hyps, refs, sources = [], [], []
    test_path = os.path.join(args.data_dir, args.test_file)
    print(f"åŠ è½½æµ‹è¯•æ•°æ®: {test_path}")
    
    with open(test_path, 'r', encoding='utf-8') as f:
        for line in tqdm(f, desc="ç¿»è¯‘ä¸­"):
            pair = json.loads(line)
            source_text = pair['zh']
            ref_text = pair['en']
            
            prompted_text = "translate Chinese to English: " + source_text

            input_ids = tokenizer(prompted_text, return_tensors='pt', max_length=args.max_len, truncation=True).input_ids.to(args.device)
            
            outputs = model.generate(
                input_ids=input_ids, 
                max_length=args.max_len,
                num_beams=args.num_beams,
                early_stopping=True,
                no_repeat_ngram_size=2,
                length_penalty=1.0
            )
            hyp = tokenizer.decode(outputs[0], skip_special_tokens=True)

            hyps.append(hyp)
            sources.append(source_text)
            refs.append([ref_text])

    bleu = sacrebleu.corpus_bleu(hyps, list(zip(*refs)))
    print(f"\n{'='*50}")
    print(f"T5 BLEU Score: {bleu.score:.2f}")
    print(f"{'='*50}\n")
    
    print("ç¤ºä¾‹ç¿»è¯‘ï¼ˆå‰ 5 æ¡ï¼‰:")
    for i in range(min(5, len(hyps))):
        print(f"\n--- æ ·æœ¬ {i+1} ---")
        print(f"æº: {sources[i]}")
        print(f"é¢„æµ‹: {hyps[i]}")
        print(f"å‚è€ƒ: {refs[i][0]}")

if __name__ == '__main__':
    main()