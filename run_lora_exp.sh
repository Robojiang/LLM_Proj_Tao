#!/bin/bash

# è®¾ç½®é‡åˆ°é”™è¯¯å³åœæ­¢
set -e

# æ¿€æ´»ç¯å¢ƒ (æ ¹æ®ä½ çš„ç¯å¢ƒè·¯å¾„è°ƒæ•´)
source /opt/miniconda3/etc/profile.d/conda.sh
conda activate llm

cd /mnt/afs/250010074/llm/LLM_Proj_Tao

# æŒ‡å®š GPU
export CUDA_VISIBLE_DEVICES=0
# è§£å†³ numexpr è­¦å‘Šï¼Œå…è®¸ä½¿ç”¨æ›´å¤š CPU æ ¸å¿ƒè¿›è¡Œè®¡ç®—
export NUMEXPR_MAX_THREADS=192

echo "ğŸš€ å¼€å§‹è¿è¡Œ LoRA Rank å¯¹æ¯”å®éªŒ..."

# ====================================================
# å®éªŒ 1: Rank = 8 (åŸºçº¿)
# ====================================================
echo "Running Experiment 1: LoRA Rank = 8 (Baseline)"
python finetune_t5_raw.py \
    model.lora.r=8 \
    model.lora.lora_alpha=16 \
    wandb.run_name=mt5-lora-r8

# ====================================================
# å®éªŒ 2: Rank = 32 (æå‡å®¹é‡)
# ====================================================
echo "Running Experiment 2: LoRA Rank = 32"
python finetune_t5_raw.py \
    model.lora.r=16 \
    model.lora.lora_alpha=32 \
    wandb.run_name=mt5-lora-r32

# ====================================================
# å®éªŒ 3: Rank = 64 (é«˜å®¹é‡)
# ====================================================
echo "Running Experiment 3: LoRA Rank = 64"
python finetune_t5_raw.py \
    model.lora.r=32 \
    model.lora.lora_alpha=64 \
    wandb.run_name=mt5-lora-r64

echo "ğŸ‰ æ‰€æœ‰ LoRA å®éªŒè¿è¡Œå®Œæ¯•ï¼è¯·å» WandB æŸ¥çœ‹ train_loss å’Œ val_loss çš„ä¸‹é™æ›²çº¿å¯¹æ¯”ã€‚"
