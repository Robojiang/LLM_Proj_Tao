#!/bin/bash

# é‡åˆ°é”™è¯¯ç«‹å³åœæ­¢
set -e

# è®¾ç½® GPU (æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹)
export CUDA_VISIBLE_DEVICES=0
# è§£å†³ numexpr è­¦å‘Šï¼Œå…è®¸ä½¿ç”¨æ›´å¤š CPU æ ¸å¿ƒè¿›è¡Œè®¡ç®—
export NUMEXPR_MAX_THREADS=192

cd /mnt/afs/250010074/llm/LLM_Proj_Tao

# source conda.sh è®© conda activate èƒ½ç”¨
source /opt/miniconda3/etc/profile.d/conda.sh

conda activate llm

echo "ğŸš€ å¼€å§‹è¿è¡Œ Transformer å…¨é¢è¯„ä¼°å®éªŒ..."

# ============================================================
# Group 1: æ¶æ„æ¶ˆè (Architecture Ablation)
# å˜é‡: pos_embedding_type, norm_type
# ============================================================

echo ">>> [Group 1] Running Baseline: Absolute Pos + LayerNorm"
python train.py \
    model=transformer \
    model.pos_embedding_type=absolute \
    model.norm_type=layernorm \

echo ">>> [Group 1] Running Exp: Relative Pos + LayerNorm"
python train.py \
    model=transformer \
    model.pos_embedding_type=relative \
    model.norm_type=layernorm \

echo ">>> [Group 1] Running Exp: Relative Pos + RMSNorm (Best Config)"
python train.py \
    model=transformer \
    model.pos_embedding_type=relative \
    model.norm_type=rmsnorm \


# ============================================================
# Group 2: Batch Size æ•æ„Ÿæ€§ (Batch Size Sensitivity)
# å˜é‡: train.batch_size
# åŸºå‡†: Batch=128 (å·²åœ¨ä¸Šé¢ Group 1 çš„ç¬¬ä¸‰ä¸ªå®éªŒè·‘è¿‡ï¼Œè¿™é‡Œä¸å†é‡å¤è·‘)
# ============================================================

echo ">>> [Group 2] Running Batch Size = 64"
python train.py \
    model=transformer \
    model.pos_embedding_type=relative \
    model.norm_type=rmsnorm \
    train.batch_size=64 \
   
echo ">>> [Group 2] Running Batch Size = 256"
python train.py \
    model=transformer \
    model.pos_embedding_type=relative \
    model.norm_type=rmsnorm \
    train.batch_size=256 \
    


# ============================================================
# Group 3: å­¦ä¹ ç‡æ•æ„Ÿæ€§ (Learning Rate Sensitivity)
# å˜é‡: train.lr
# åŸºå‡†: LR=5e-4 (å·²åœ¨ Group 1 è·‘è¿‡)
# ============================================================

echo ">>> [Group 3] Running LR = 5e-5"
python train.py \
    model=transformer \
    model.pos_embedding_type=relative \
    model.norm_type=rmsnorm \
    train.lr=0.00005 \

echo ">>> [Group 3] Running LR = 1.5e-4"
python train.py \
    model=transformer \
    model.pos_embedding_type=relative \
    model.norm_type=rmsnorm \
    train.lr=0.00015 \


# ============================================================
# Group 4: æ¨¡å‹è§„æ¨¡ (Model Scales)
# å˜é‡: d_model, nhead, num_layers, dim_feedforward
# åŸºå‡†: Base (d=512, h=8, l=6) (å·²åœ¨ Group 1 è·‘è¿‡)
# ============================================================

echo ">>> [Group 4] Running Model Scale: Tiny (d=256, L=3)"
python train.py \
    model=transformer \
    model.pos_embedding_type=relative \
    model.norm_type=rmsnorm \
    model.d_model=256 \
    model.dim_feedforward=1024 \
    model.nhead=4 \
    model.num_encoder_layers=3 \
    model.num_decoder_layers=3 \

echo ">>> [Group 4] Running Model Scale: Big (d=768, L=6)"
# æ³¨æ„: H100ä¸Šè·‘è¿™ä¸ªæ²¡é—®é¢˜ï¼Œå¦‚æœæ˜¾å­˜ä¸å¤Ÿè¯·å‡å° BatchSize
python train.py \
    model=transformer \
    model.pos_embedding_type=relative \
    model.norm_type=rmsnorm \
    model.d_model=768 \
    model.dim_feedforward=3072 \
    model.nhead=12 \
    model.num_encoder_layers=6 \
    model.num_decoder_layers=6 \

echo "ğŸ‰ æ‰€æœ‰ Transformer å®éªŒè¿è¡Œå®Œæ¯•ï¼"