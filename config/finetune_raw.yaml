# 实验和设备配置
experiment: mt5-finetune-raw
seed: 42
device: cuda

# 数据配置
data:
  dir: ./data  # 修改为原始数据路径
  train_file: train_100k.jsonl
  valid_file: valid.jsonl
  max_len: 80 # 与你的预处理保持一致

model:
  name: google/mt5-small # mt5-small | mt5-base | mt5-large
  finetune_method: lora # 微调方法: full | lora
  # LoRA 配置 (仅在 finetune_method=lora 时生效)
  lora:
    r: 8
    lora_alpha: 16
    lora_dropout: 0.1
    target_modules: ["q", "v", "k", "o"]  # 覆盖更多模块 # mT5的attention模块名称略有不同

train:
  epochs: 20
  batch_size: 12
  lr: 1e-4
  scheduler: linear # linear | constant
  warmup_steps: 1000
  resume_from: null  # 指定 checkpoint 文件路径，例如 ./runs/t5_finetune/checkpoint_epoch_3.pt
  save_every: 3      # 每 3 个 epoch 保存一个 checkpoint

# W&B 配置
wandb:
  enable: True
  project: nmt-course-finetune-raw # 为微调任务创建新项目
  run_name: mt5-small-${model.finetune_method}-finetune-raw