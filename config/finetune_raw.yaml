# 实验和设备配置
experiment: mt5-finetune-raw
seed: 42
device: cuda

# 数据配置
data:
  dir: ./data  # 修改为原始数据路径
  train_file: train_100k.jsonl
  valid_file: valid.jsonl
  max_len: 80 # 与你的预处理保持一致

model:
  name: google/mt5-small # mt5-small | mt5-base | mt5-large
  finetune_method: lora # 微调方法: full | lora
  # LoRA 配置 (仅在 finetune_method=lora 时生效)
  lora:
    r: 32
    lora_alpha: 64
    lora_dropout: 0.2 # 增加 dropout 防止过拟合
    target_modules: ["q", "v", "k", "o"]  # 覆盖更多模块 # mT5的attention模块名称略有不同

train:
  epochs: 50 # 减少训练轮数
  batch_size: 128
  lr: 5e-5 # 降低学习率
  weight_decay: 0.1 # 增加权重衰减
  scheduler: linear # linear | constant
  warmup_steps: 1000
  resume_from: null  # 指定 checkpoint 文件路径，例如 ./runs/t5_finetune/checkpoint_epoch_3.pt
  save_every: 10      # 每 10 个 epoch 保存一个 checkpoint

# W&B 配置
wandb:
  enable: True
  project: nmt-course-finetune-raw # 为微调任务创建新项目
  run_name: mt5-small-${model.finetune_method}