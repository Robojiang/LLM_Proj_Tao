import os, json, torch
from torch.utils.data import Dataset, DataLoader
from transformers import MT5Tokenizer, MT5ForConditionalGeneration, get_linear_schedule_with_warmup
from torch.optim import AdamW
from tqdm import tqdm
import hydra
from omegaconf import DictConfig, OmegaConf
from hydra.utils import to_absolute_path
import wandb
from datetime import datetime

# --- PEFT (LoRA) æ”¯æŒ ---
try:
    from peft import get_peft_model, LoraConfig, TaskType
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False

class T5NMTDataset(Dataset):
    def __init__(self, jsonl_path, max_len=128):
        """ä½¿ç”¨åŸå§‹æ•°æ®æ ¼å¼"""
        self.data = []
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                pair = json.loads(line)
                # ç›´æ¥ä½¿ç”¨åŸå§‹çš„ä¸­è‹±æ–‡å¥å­
                src = pair['zh']
                tgt = pair['en']
                self.data.append((src, tgt))
        self.max_len = max_len
        print(f"åŠ è½½äº† {len(self.data)} æ¡æ•°æ®")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return {'src': self.data[idx][0], 'tgt': self.data[idx][1]}

def collate_fn(batch, tokenizer, max_len):
    """åŠ¨æ€å¡«å……çš„ collate å‡½æ•°"""
    srcs = ["translate Chinese to English: " + item['src'] for item in batch]
    tgts = [item['tgt'] for item in batch]
    
    # ç¼–ç æºå¥å­å’Œç›®æ ‡å¥å­
    src_enc = tokenizer(srcs, max_length=max_len, padding=True, truncation=True, return_tensors='pt')
    tgt_enc = tokenizer(tgts, max_length=max_len, padding=True, truncation=True, return_tensors='pt')
    
    # å°†ç›®æ ‡çš„ padding token æ›¿æ¢ä¸º -100ï¼ˆå¿½ç•¥ loss è®¡ç®—ï¼‰
    labels = tgt_enc['input_ids'].clone()
    labels[labels == tokenizer.pad_token_id] = -100
    
    return {
        'input_ids': src_enc['input_ids'],
        'attention_mask': src_enc['attention_mask'],
        'labels': labels
    }

@hydra.main(config_path="config", config_name="finetune_raw", version_base=None)
def main(cfg: DictConfig):
    # --- è·¯å¾„å’Œè®¾å¤‡ ---
    data_dir = to_absolute_path(cfg.data.dir)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_name_safe = cfg.model.name.replace('/', '-')
    output_dir = os.path.join("./runs", f"{timestamp}_{cfg.experiment}_{model_name_safe}_{cfg.model.finetune_method}_raw")
    os.makedirs(output_dir, exist_ok=True)
    
    device = cfg.device if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # ä¿å­˜é…ç½®
    with open(os.path.join(output_dir, 'config.yaml'), 'w') as f:
        f.write(OmegaConf.to_yaml(cfg))

    # --- W&B åˆå§‹åŒ– ---
    if cfg.wandb.enable:
        wandb.init(
            project=cfg.wandb.project,
            name=cfg.wandb.run_name or os.path.basename(output_dir),
            config=OmegaConf.to_container(cfg, resolve=True)
        )

    # --- åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print(f"åŠ è½½æ¨¡å‹: {cfg.model.name}")
    tokenizer = MT5Tokenizer.from_pretrained(cfg.model.name)
    model = MT5ForConditionalGeneration.from_pretrained(cfg.model.name,use_safetensors=True)

    # --- å¾®è°ƒæ–¹æ³•é€‰æ‹© ---
    if cfg.model.finetune_method == 'lora':
        if not PEFT_AVAILABLE:
            raise ImportError("è¯·å®‰è£… peft åº“: pip install peft")
        print("ğŸš€ ä½¿ç”¨ LoRA å¾®è°ƒ...")
        peft_config = LoraConfig(
            task_type=TaskType.SEQ_2_SEQ_LM,
            r=cfg.model.lora.r,
            lora_alpha=cfg.model.lora.lora_alpha,
            lora_dropout=cfg.model.lora.lora_dropout,
            target_modules=list(cfg.model.lora.target_modules)
        )
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
    else:
        print("ğŸš€ ä½¿ç”¨å…¨é‡å¾®è°ƒ...")
        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"å¯è®­ç»ƒå‚æ•°é‡: {num_params:,}")

    model.to(device)

    # --- æ•°æ®åŠ è½½ ---
    print(f"åŠ è½½è®­ç»ƒæ•°æ®: {cfg.data.train_file}")
    train_ds = T5NMTDataset(os.path.join(data_dir, cfg.data.train_file), cfg.data.max_len)
    print(f"åŠ è½½éªŒè¯æ•°æ®: {cfg.data.valid_file}")
    valid_ds = T5NMTDataset(os.path.join(data_dir, cfg.data.valid_file), cfg.data.max_len)
    
    train_loader = DataLoader(
        train_ds, 
        batch_size=cfg.train.batch_size, 
        shuffle=True,
        collate_fn=lambda batch: collate_fn(batch, tokenizer, cfg.data.max_len)
    )
    valid_loader = DataLoader(
        valid_ds, 
        batch_size=cfg.train.batch_size,
        collate_fn=lambda batch: collate_fn(batch, tokenizer, cfg.data.max_len)
    )

    # --- ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ---
    optimizer = AdamW(model.parameters(), lr=cfg.train.lr)
    total_steps = len(train_loader) * cfg.train.epochs
    if cfg.train.scheduler == 'linear':
        scheduler = get_linear_schedule_with_warmup(
            optimizer, num_warmup_steps=cfg.train.warmup_steps, num_training_steps=total_steps
        )
    else:
        scheduler = None

    # --- æ¢å¤è®­ç»ƒ ---
    start_epoch = 0
    best_val_loss = float('inf')
    if cfg.train.resume_from:
        checkpoint = torch.load(to_absolute_path(cfg.train.resume_from), map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if scheduler and checkpoint.get('scheduler_state_dict'):
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        print(f"ğŸ”„ æ¢å¤è®­ç»ƒï¼Œä» epoch {start_epoch} å¼€å§‹ï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")

    # --- è®­ç»ƒå¾ªç¯ ---
    print(f"\nå¼€å§‹è®­ç»ƒï¼Œå…± {cfg.train.epochs} ä¸ª epochï¼Œæ¯ {cfg.train.save_every} ä¸ª epoch ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹\n")
    for epoch in range(start_epoch, cfg.train.epochs):
        model.train()
        total_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{cfg.train.epochs}")
        for batch in pbar:
            optimizer.zero_grad()
            
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            if scheduler:
                scheduler.step()

            total_loss += loss.item()
            current_lr = optimizer.param_groups[0]['lr']
            pbar.set_postfix(loss=f"{loss.item():.4f}", lr=f"{current_lr:.2e}")
            
            if cfg.wandb.enable:
                wandb.log({"step_loss": loss.item(), "lr": current_lr})

        avg_train_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1} - å¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")

        # --- éªŒè¯ ---
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in tqdm(valid_loader, desc="éªŒè¯ä¸­"):
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                val_loss += outputs.loss.item()
        avg_val_loss = val_loss / len(valid_loader)
        print(f"Epoch {epoch+1} - éªŒè¯æŸå¤±: {avg_val_loss:.4f}\n")
        
        if cfg.wandb.enable:
            wandb.log({"epoch": epoch+1, "train_loss": avg_train_loss, "val_loss": avg_val_loss})

        # --- ä¿å­˜æœ€ä½³æ¨¡å‹ ---
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model_path = os.path.join(output_dir, "best_model")
            model.save_pretrained(best_model_path)
            tokenizer.save_pretrained(best_model_path)
            print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {best_model_path} (éªŒè¯æŸå¤±: {avg_val_loss:.4f})\n")

        # --- ä¿å­˜ checkpoint ---
        if (epoch + 1) % cfg.train.save_every == 0:
            checkpoint_path = os.path.join(output_dir, f"checkpoint_epoch_{epoch+1}.pt")
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                'best_val_loss': best_val_loss
            }, checkpoint_path)
            print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}\n")

    # --- ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    final_model_path = os.path.join(output_dir, "final_model")
    model.save_pretrained(final_model_path)
    tokenizer.save_pretrained(final_model_path)
    print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")
    if cfg.wandb.enable:
        wandb.finish()

if __name__ == '__main__':
    main()